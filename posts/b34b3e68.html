<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"dra-tammer.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":40,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="d2l（李沐）课程精简笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学深度学习">
<meta property="og:url" content="https://dra-tammer.github.io/posts/b34b3e68.html">
<meta property="og:site_name" content="Toothless">
<meta property="og:description" content="d2l（李沐）课程精简笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/resnet_block.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/edge_dot.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/encoderDecoder.png">
<meta property="article:published_time" content="2024-11-29T09:57:23.000Z">
<meta property="article:modified_time" content="2025-01-17T14:26:58.560Z">
<meta property="article:author" content="tammer">
<meta property="article:tag" content="技术">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/resnet_block.png">


<link rel="canonical" href="https://dra-tammer.github.io/posts/b34b3e68.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://dra-tammer.github.io/posts/b34b3e68.html","path":"posts/b34b3e68.html","title":"动手学深度学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>动手学深度学习 | Toothless</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Toothless" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Toothless</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">world for tammer</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">35</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">43</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">动手学深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E7%BB%84"><span class="nav-number">1.1.1.</span> <span class="nav-text">创建数组</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.3.</span> <span class="nav-text">小批量随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB"><span class="nav-number">1.4.</span> <span class="nav-text">回归 &amp; 分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB-%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="nav-number">1.5.</span> <span class="nav-text">从回归到多类分类-均方损失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">1.6.</span> <span class="nav-text">Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.7.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%EF%BC%88L2-Loss%EF%BC%89"><span class="nav-number">1.7.1.</span> <span class="nav-text">平方损失（L2 Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9D%E5%AF%B9%E5%80%BC%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88L1-Loss%EF%BC%89"><span class="nav-number">1.7.2.</span> <span class="nav-text">绝对值损失函数（L1 Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%B2%81%E6%A3%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Huber%E2%80%99s-Robust-Loss%EF%BC%89"><span class="nav-number">1.7.3.</span> <span class="nav-text">鲁棒损失函数（Huber’s Robust Loss）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88CrossEntropyLoss%EF%BC%89"><span class="nav-number">1.7.4.</span> <span class="nav-text">交叉熵损失函数（CrossEntropyLoss）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="nav-number">1.8.</span> <span class="nav-text">图像分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iter"><span class="nav-number">1.9.</span> <span class="nav-text">iter()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89"><span class="nav-number">1.10.</span> <span class="nav-text">感知机（MLP）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">1.10.1.</span> <span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.11.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82-%E5%8D%95%E5%88%86%E7%B1%BB"><span class="nav-number">1.11.1.</span> <span class="nav-text">单隐藏层-单分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.11.2.</span> <span class="nav-text">sigmoid激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tanh%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.11.3.</span> <span class="nav-text">Tanh激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.11.4.</span> <span class="nav-text">ReLU激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="nav-number">1.11.5.</span> <span class="nav-text">多类分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">1.11.6.</span> <span class="nav-text">多隐藏层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A2%9E%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E5%B1%82%E6%95%B0%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="nav-number">1.12.</span> <span class="nav-text">深度神经网络为什么要增加隐藏层的层数，而不是神经元的个数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="nav-number">1.13.</span> <span class="nav-text">训练误差和泛化误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%EF%BC%88k-fold%EF%BC%89"><span class="nav-number">1.14.</span> <span class="nav-text">K-折交叉验证（k-fold）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="nav-number">1.15.</span> <span class="nav-text">模型容量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="nav-number">1.16.</span> <span class="nav-text">权重衰退</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="nav-number">1.17.</span> <span class="nav-text">丢弃法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E7%9A%84%E5%B8%B8%E8%A7%81%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">1.18.</span> <span class="nav-text">数值稳定性的常见两个问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">1.18.1.</span> <span class="nav-text">梯度爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">1.18.2.</span> <span class="nav-text">梯度消失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A"><span class="nav-number">1.19.</span> <span class="nav-text">让训练更加稳定</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%88%E7%90%86%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.19.1.</span> <span class="nav-text">合理的权重初始和激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0"><span class="nav-number">1.19.1.1.</span> <span class="nav-text">让每层的方差是一个常数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.19.1.2.</span> <span class="nav-text">权重初始化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.20.</span> <span class="nav-text">前向传播和反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.20.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.20.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.21.</span> <span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E4%B8%AA%E5%8E%9F%E5%88%99"><span class="nav-number">1.21.1.</span> <span class="nav-text">两个原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3"><span class="nav-number">1.21.2.</span> <span class="nav-text">二维交叉相关</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4%E5%92%8C%E4%B8%89%E7%BB%B4%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3"><span class="nav-number">1.21.3.</span> <span class="nav-text">一维和三维交叉相关</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6"><span class="nav-number">1.21.4.</span> <span class="nav-text">输入维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="nav-number">1.21.5.</span> <span class="nav-text">感受野</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="nav-number">1.21.6.</span> <span class="nav-text">填充和步幅</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">1.21.6.1.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E5%B9%85"><span class="nav-number">1.21.6.2.</span> <span class="nav-text">步幅</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">1.21.7.</span> <span class="nav-text">多输入输出通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="nav-number">1.21.8.</span> <span class="nav-text">多输入通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">1.21.9.</span> <span class="nav-text">多个输出通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.21.10.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF-%E4%BD%9C%E7%94%A8"><span class="nav-number">1.21.11.</span> <span class="nav-text">背景&amp;作用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96"><span class="nav-number">1.21.11.1.</span> <span class="nav-text">二维最大池化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.21.11.2.</span> <span class="nav-text">平均池化层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.22.</span> <span class="nav-text">正向传播和反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="nav-number">1.22.1.</span> <span class="nav-text">卷积层可学习的参数的个数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Alexnet"><span class="nav-number">1.23.</span> <span class="nav-text">Alexnet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%96%B9%E6%B3%95%E8%AE%BA%E7%9A%84%E6%94%B9%E5%8F%98%EF%BC%9A"><span class="nav-number">1.23.1.</span> <span class="nav-text">计算机视觉方法论的改变：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NiN"><span class="nav-number">1.24.</span> <span class="nav-text">NiN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NiN%E5%9D%97"><span class="nav-number">1.24.1.</span> <span class="nav-text">NiN块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.24.2.</span> <span class="nav-text">全局池化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">1.25.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">1.26.</span> <span class="nav-text">批量归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.26.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="nav-number">1.26.2.</span> <span class="nav-text">批量归一化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-number">1.27.</span> <span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97%E7%BB%93%E6%9E%84%EF%BC%9A"><span class="nav-number">1.27.1.</span> <span class="nav-text">残差块结构：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C"><span class="nav-number">1.28.</span> <span class="nav-text">单机多卡并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">1.28.1.</span> <span class="nav-text">数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="nav-number">1.28.2.</span> <span class="nav-text">模型并行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF"><span class="nav-number">1.29.</span> <span class="nav-text">数据增广</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%EF%BC%88fine-tuning%EF%BC%89"><span class="nav-number">1.30.</span> <span class="nav-text">微调（fine-tuning）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">1.30.1.</span> <span class="nav-text">网络架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.30.2.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.30.3.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E7%94%A8%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D"><span class="nav-number">1.30.4.</span> <span class="nav-text">重用分类器权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E4%B8%80%E4%BA%9B%E5%B1%82"><span class="nav-number">1.30.5.</span> <span class="nav-text">固定一些层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-number">1.31.</span> <span class="nav-text">目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%B9%E7%BC%98%E6%A1%86"><span class="nav-number">1.31.1.</span> <span class="nav-text">边缘框</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.31.2.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%94%9A%E6%A1%86"><span class="nav-number">1.32.</span> <span class="nav-text">锚框</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IoU"><span class="nav-number">1.32.1.</span> <span class="nav-text">IoU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%8B%E4%BA%88%E9%94%9A%E6%A1%86%E6%A0%87%E5%8F%B7"><span class="nav-number">1.32.2.</span> <span class="nav-text">赋予锚框标号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%EF%BC%88NMS%EF%BC%89%E8%BE%93%E5%87%BA"><span class="nav-number">1.32.3.</span> <span class="nav-text">使用非极大值抑制（NMS）输出</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">1.33.</span> <span class="nav-text">编码器&amp;解码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E6%96%B0%E8%80%83%E5%AF%9FCNN"><span class="nav-number">1.33.1.</span> <span class="nav-text">重新考察CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E6%96%B0%E8%80%83%E5%AF%9FRNN"><span class="nav-number">1.33.2.</span> <span class="nav-text">重新考察RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-number">1.33.3.</span> <span class="nav-text">架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.34.</span> <span class="nav-text">循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%A8%A1%E6%9D%BF"><span class="nav-number">1.35.</span> <span class="nav-text">代码模板</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#d2l%E5%9B%BE%E7%89%87%E4%B8%8D%E6%98%BE%E7%A4%BA"><span class="nav-number">1.35.1.</span> <span class="nav-text">d2l图片不显示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="nav-number">1.35.2.</span> <span class="nav-text">训练</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="tammer"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">tammer</p>
  <div class="site-description" itemprop="description">我忘记一切悲剧，所见皆是奇迹</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Dra-Tammer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Dra-Tammer" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dra-tammer.github.io/posts/b34b3e68.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="tammer">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toothless">
      <meta itemprop="description" content="我忘记一切悲剧，所见皆是奇迹">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="动手学深度学习 | Toothless">
      <meta itemprop="description" content="d2l（李沐）课程精简笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          动手学深度学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-29 17:57:23" itemprop="dateCreated datePublished" datetime="2024-11-29T17:57:23+08:00">2024-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-17 22:26:58" itemprop="dateModified" datetime="2025-01-17T22:26:58+08:00">2025-01-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/python/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/pytorch/python/%E5%AD%A6%E4%B9%A0/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

            <div class="post-description">d2l（李沐）课程精简笔记</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="动手学深度学习"><a href="#动手学深度学习" class="headerlink" title="动手学深度学习"></a>动手学深度学习</h1><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>N维数组是机器学习和神经网络的主要数据结构</p>
<blockquote>
<p>3-d：RGB图片（宽 * 高 * 通道 ）</p>
</blockquote>
<h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><ol>
<li>形状</li>
<li>每个形状的数据类型</li>
<li>每个元素的值</li>
</ol>
<blockquote>
<p>对于深度学习的话，64位浮点数计算的比较慢，我们一般使用32位浮点数</p>
</blockquote>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归可以看成一个单层的神经网络</p>
<h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><p>SGD</p>
<blockquote>
<p>深度学习默认的求解算法</p>
</blockquote>
<p>在整个训练集上算梯度太贵</p>
<p>随机采样b个样本来近似损失</p>
<p>两个重要的超参数是批量大小和学习率</p>
<h2 id="回归-分类"><a href="#回归-分类" class="headerlink" title="回归 &amp; 分类"></a>回归 &amp; 分类</h2><ul>
<li><p>回归估计一个连续值</p>
<p>单连续数值输出，自然区间，跟真实值的区别作为损失</p>
</li>
<li><p>分类预测一个离散类别</p>
<p>通常是多个输出，输出i是预测为第i类的置信度</p>
</li>
</ul>
<p>MNIST：手写数字识别</p>
<p>ImageNet：自然物体分类</p>
<h2 id="从回归到多类分类-均方损失"><a href="#从回归到多类分类-均方损失" class="headerlink" title="从回归到多类分类-均方损失"></a>从回归到多类分类-均方损失</h2><ul>
<li>对类别进行一位有效编码</li>
<li>最大值作为预测</li>
<li>需要更置信的识别正确类（大余量）</li>
<li>输出匹配概率</li>
<li>概率y和yba的区别作为损失</li>
</ul>
<blockquote>
<p>指数的好处是不管是什么值都能给他整成非负</p>
</blockquote>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><ul>
<li>多类分类模型</li>
<li>使用Softmax操作了得到每个类的预测置信度</li>
<li>使用交叉熵来衡量预测和标号的区别</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="平方损失（L2-Loss）"><a href="#平方损失（L2-Loss）" class="headerlink" title="平方损失（L2 Loss）"></a>平方损失（L2 Loss）</h3><p>l(y, ypie) = 1 / 2(y - ypie)^2</p>
<h3 id="绝对值损失函数（L1-Loss）"><a href="#绝对值损失函数（L1-Loss）" class="headerlink" title="绝对值损失函数（L1 Loss）"></a>绝对值损失函数（L1 Loss）</h3><p>预测值和真实值比较远的时候，梯度也是一个常数，更新的幅度不变，稳定性好一点</p>
<h3 id="鲁棒损失函数（Huber’s-Robust-Loss）"><a href="#鲁棒损失函数（Huber’s-Robust-Loss）" class="headerlink" title="鲁棒损失函数（Huber’s Robust Loss）"></a>鲁棒损失函数（Huber’s Robust Loss）</h3><h3 id="交叉熵损失函数（CrossEntropyLoss）"><a href="#交叉熵损失函数（CrossEntropyLoss）" class="headerlink" title="交叉熵损失函数（CrossEntropyLoss）"></a>交叉熵损失函数（CrossEntropyLoss）</h3><h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p><code>fashion-mnist</code></p>
<p>测试数据集来预测模型的好坏</p>
<p>训练集的话是要随机的，但是测试集的话就无所谓了</p>
<p>一个常见的性能瓶颈，GPU训练的比数据读取的要快</p>
<h2 id="iter"><a href="#iter" class="headerlink" title="iter()"></a>iter()</h2><p>生成迭代器，逐个地遍历一个可迭代对象</p>
<h2 id="感知机（MLP）"><a href="#感知机（MLP）" class="headerlink" title="感知机（MLP）"></a>感知机（MLP）</h2><p>做一个二分类的问题</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>不能拟合XOR函数，只能产生线性分割面</p>
<p>XOR（x =  1, y = 1 ; x = -1, y = -1 =&gt; -1）（x =  1, y = -1 ; x = -1, y = 1 =&gt; 1）</p>
<p>解决方法：多层感知机</p>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="单隐藏层-单分类"><a href="#单隐藏层-单分类" class="headerlink" title="单隐藏层-单分类"></a>单隐藏层-单分类</h3><p>隐藏层的大小是一个超参数，因为输入输出不是我们能改的</p>
<p>多层感知机需要有一个非线性激活函数，否则就还是线性模型，也就是单层感知机</p>
<p>引入激活函数的本质是引入非线性性 </p>
<h3 id="sigmoid激活函数"><a href="#sigmoid激活函数" class="headerlink" title="sigmoid激活函数"></a>sigmoid激活函数</h3><h3 id="Tanh激活函数"><a href="#Tanh激活函数" class="headerlink" title="Tanh激活函数"></a>Tanh激活函数</h3><h3 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h3><blockquote>
<p>指数运算是一个很贵的东西</p>
</blockquote>
<h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p>softmax回归加上一个隐藏层就会变成多层感知机</p>
<p>softmax把所有的输入拉到一个0和1之间的区域，而且y1加到yk等于1</p>
<h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><ul>
<li>隐藏层数</li>
<li>每层隐藏层的大小</li>
</ul>
<blockquote>
<p>输出层是不需要激活函数的，因为激活函数是为了避免隐藏层的塌陷</p>
</blockquote>
<p>机器学习可以看作一个压缩的过程，如果维度太高的话，要逐渐压缩</p>
<p>可以先扩张再压缩 </p>
<blockquote>
<p>多层感知机使用隐藏层和激活函数来得到非线性模型</p>
<p>常用的激活函数由sigmoid，tanh，relu</p>
<p>使用softmax来处理多类分类</p>
<p>超参数为隐藏层数，和各个隐藏层的大小</p>
</blockquote>
<blockquote>
<p>一个箭头算成一个层</p>
</blockquote>
<h2 id="深度神经网络为什么要增加隐藏层的层数，而不是神经元的个数"><a href="#深度神经网络为什么要增加隐藏层的层数，而不是神经元的个数" class="headerlink" title="深度神经网络为什么要增加隐藏层的层数，而不是神经元的个数"></a>深度神经网络为什么要增加隐藏层的层数，而不是神经元的个数</h2><p>因为层数更深，拟合的效果更好，所以叫深度神经网络，如果就一层特别宽的一层，一口吃个大胖子，非常容易过拟合</p>
<h2 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h2><p>关注的是泛化误差</p>
<p>训练数据集：训练模型参数</p>
<p>验证数据集：一个用来评估模型好坏的数据集，选择模型的超参数</p>
<p>非大数据集上通常使用k-折交叉验证</p>
<ul>
<li>例如拿出50%的训练数据</li>
<li>不要跟训练数据混在一起（常犯错误）</li>
</ul>
<p>测试数据集：只用一次的数据集，不能用来调整超参数</p>
<ul>
<li>未来的考试</li>
<li>我出价的房子的实际成交价</li>
<li>用在kaggle私有排行榜中的数据集</li>
</ul>
<h2 id="K-折交叉验证（k-fold）"><a href="#K-折交叉验证（k-fold）" class="headerlink" title="K-折交叉验证（k-fold）"></a>K-折交叉验证（k-fold）</h2><ul>
<li><p>在没有足够多数据时使用</p>
</li>
<li><p>算法：</p>
<ul>
<li><p>将训练数据分割成k块</p>
</li>
<li><p>for i -&gt; k</p>
<p>使用第i块作为验证数据集，其余的作为训练数据集</p>
</li>
</ul>
</li>
<li><p>报告k个数据集误差的平均</p>
</li>
<li><p>常用：k = 5或者10</p>
</li>
</ul>
<h2 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h2><ul>
<li>拟合各种函数的能力</li>
<li>低容量的模型难以拟合训练数据</li>
<li>高容量的模型可以记住所有的训练数据</li>
</ul>
<blockquote>
<p>过拟合不是一件坏的事儿，首先模型容量要足够大，之后通过各种手段控制模型容量，最后得到泛化误差的下降</p>
</blockquote>
<blockquote>
<p>神经网络是一个很灵活的语言，可编程性很好，但是不好理解</p>
</blockquote>
<h2 id="权重衰退"><a href="#权重衰退" class="headerlink" title="权重衰退"></a>权重衰退</h2><p>防止过拟合</p>
<h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>dropout三个值，0.1，0.5，0.9</p>
<p>更好的解决过拟合的问题</p>
<p>在层之间加入噪音，随机加上噪音，等价于Tikhnov</p>
<p>无偏差加入噪音，但是不影响期望</p>
<p>通常将丢弃法作用在隐藏全连接层的输出</p>
<p>正则项只在训练中使用，他们影响模型参数的更新（确定w，b），更新参数的时候，使得模型复杂度降低，在推理过程中，丢弃发直接返回输入</p>
<p>h = dropout(h)<br>这样也能保证确定性的输出</p>
<ul>
<li>丢弃发将一些输出项随机置0来控制模型复杂度</li>
<li>常作用在多层感知机的隐藏层输出上</li>
<li>丢弃概率是控制模型复杂度的超参数</li>
</ul>
<blockquote>
<p>dropout作用在全连接层的，BN作用在卷积层</p>
</blockquote>
<blockquote>
<p>cnn可以认为是一个特别的MLP</p>
</blockquote>
<h2 id="数值稳定性的常见两个问题"><a href="#数值稳定性的常见两个问题" class="headerlink" title="数值稳定性的常见两个问题"></a>数值稳定性的常见两个问题</h2><p>梯度爆炸和梯度消失，因为做了太多了矩阵乘法，梯度值浮点值</p>
<h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><p>值超出值域，对于16位浮点数尤为严重</p>
<p>对学习率敏感</p>
<p>如果学习率太大，大参数值，更大的梯度</p>
<p>如果学习率太小，训练无进展</p>
<p>可能需要在训练过程中不断调整学习率</p>
<blockquote>
<p>学习率相当于步长，而梯度则是收敛方向</p>
<p>权重是学习率乘以梯度</p>
</blockquote>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>梯度值变成0，对16位浮点数尤为严重</p>
<p>训练没有进展，不管如何选择学习率</p>
<p>对于底部曾尤为严重，仅仅顶部层训练的较好，无法让神经网络更深</p>
<h2 id="让训练更加稳定"><a href="#让训练更加稳定" class="headerlink" title="让训练更加稳定"></a>让训练更加稳定</h2><p>目标：让梯度值在合理的范围里</p>
<p>将乘法变成加法（resnet）</p>
<p>归一化：梯度归一化，梯度裁剪</p>
<h3 id="合理的权重初始和激活函数"><a href="#合理的权重初始和激活函数" class="headerlink" title="合理的权重初始和激活函数"></a>合理的权重初始和激活函数</h3><h4 id="让每层的方差是一个常数"><a href="#让每层的方差是一个常数" class="headerlink" title="让每层的方差是一个常数"></a>让每层的方差是一个常数</h4><ul>
<li>将每层的输出和梯度都看作随机变量</li>
<li>让他们的均值和方差都保持一致</li>
</ul>
<h4 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h4><ul>
<li>在合理值区间里随机初始参数</li>
<li>训练开始的时候更容易有数值不稳定<ul>
<li>远离最优解的地方损失函数表面可能很复杂</li>
<li>最优解附近表面会比较平</li>
</ul>
</li>
<li>使用（n，0.01）来初始可能对小网络没问题，但不能保证深度神经网络</li>
</ul>
<blockquote>
<p>合理的权重初始和激活函数的选取可以提升数值稳定性</p>
</blockquote>
<h2 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h2><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>简单理解就是将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><strong>反向传播仅指用于计算梯度的方法。</strong>而另一种算法，例如随机梯度下降法，才是使用该梯度来进行学习。<strong>原则上反向传播可以计算任何函数的到导数</strong></p>
<p>的确就是复合函数的链式法则</p>
<p>利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值</p>
<p>反向(自上往下)来寻找路径的</p>
<blockquote>
<p>宇宙的终极答案：42 ——–《银河系漫游指南》</p>
</blockquote>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h3 id="两个原则"><a href="#两个原则" class="headerlink" title="两个原则"></a>两个原则</h3><ul>
<li>平移不变性</li>
<li>局部性</li>
</ul>
<p> 对全连接层使用平移不变性和局部性得到卷积层</p>
<h3 id="二维交叉相关"><a href="#二维交叉相关" class="headerlink" title="二维交叉相关"></a>二维交叉相关</h3><p>虽说是卷积层但是计算的时候是交叉相关</p>
<h3 id="一维和三维交叉相关"><a href="#一维和三维交叉相关" class="headerlink" title="一维和三维交叉相关"></a>一维和三维交叉相关</h3><p>一维：文本，语言，时序序列</p>
<p>三维：视频，医学图像，气象地图</p>
<blockquote>
<p>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</p>
<p>核矩阵和偏移是可学习的参数</p>
<p>核矩阵的大小是超参数：控制局部性</p>
<p>不管输入在哪个位置，核是不变的</p>
</blockquote>
<p>卷积层可以认为一个特殊的全连接层，解决了权重随着输入的变大变得特别大</p>
<h3 id="输入维度"><a href="#输入维度" class="headerlink" title="输入维度"></a>输入维度</h3><p>输入维度都是4-D</p>
<p>（通道数，批量大小数，长，宽）</p>
<h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p>卷积层深一点，核小一点，效果更好，一般3<em>3或者5</em>5</p>
<h3 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h3><h4 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h4><p>在输入周围添加额外的行/列</p>
<h4 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h4><p>填充减小的输出大小和层数线性相关</p>
<p>步幅是指行/列的滑动步长</p>
<p>高度3，宽度2的步幅</p>
<p>步幅一般是2</p>
<p>填充和步幅都是卷积层的超参数</p>
<p>填充在输入周围添加额外的行/列，来控制输出形状的减少量，步幅是每次滑动核窗口时的步长，可以成倍减少输出形状，减少计算量。</p>
<blockquote>
<p>机器学习可以看作一个压缩的算法，永远会丢失信息的，计算机理解的信息压缩到人能理解的范围</p>
</blockquote>
<h3 id="多输入输出通道"><a href="#多输入输出通道" class="headerlink" title="多输入输出通道"></a>多输入输出通道</h3><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>每个通道都有一个卷积核，结果是所有通道卷积结果的和</p>
<h3 id="多个输出通道"><a href="#多个输出通道" class="headerlink" title="多个输出通道"></a>多个输出通道</h3><p>每个输出通道可以识别特定的模式</p>
<p>输入通道核识别并组合输入中的模式</p>
<blockquote>
<p>输出的通道数时卷积层的超参数</p>
<p>每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果</p>
<p>每个输出通道有独立的三维卷积核</p>
</blockquote>
<p>通道之间是不共享参数，每个通道学习一个特定的模式</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><h3 id="背景-作用"><a href="#背景-作用" class="headerlink" title="背景&amp;作用"></a>背景&amp;作用</h3><blockquote>
<p>卷积层对位置信息太敏感</p>
</blockquote>
<p>需要一定程度的平移不变性，照明，物体位置，比例，外观等等因图像而异</p>
<h4 id="二维最大池化"><a href="#二维最大池化" class="headerlink" title="二维最大池化"></a>二维最大池化</h4><p>返回滑动窗口中的最大值</p>
<p>池化层和卷积层类似，都具有填充和步幅</p>
<p>没有可以学习的参数，没有kernel，k</p>
<p>最每个输入通道应用池化层以获得相应的输出通道，不会作融合</p>
<p>输出通道数=输入通道数</p>
<h4 id="平均池化层"><a href="#平均池化层" class="headerlink" title="平均池化层"></a>平均池化层</h4><p>最大池化层：每个窗口中间最强的模式信号</p>
<p>平均池化层：将最大池化层中的“最大”操作替换为“平均”</p>
<blockquote>
<p>缓解卷积层对位置的敏感性</p>
<p>同样有窗口大小，填充，步幅作为超参数，对每个通道作用直接输出</p>
</blockquote>
<p>池化层用的越来越少</p>
<h2 id="正向传播和反向传播"><a href="#正向传播和反向传播" class="headerlink" title="正向传播和反向传播"></a>正向传播和反向传播</h2><ul>
<li><strong>前向传播（Forward Propagation）</strong>前向传播就是从input，经过一层层的layer，不断计算每一层的z和a，最后得到输出y^ 的过程，计算出了y^，就可以根据它和真实值y的差别来计算损失（loss）。</li>
<li><strong>反向传播（Backward Propagation）</strong>反向传播就是根据损失函数L(y^,y)来反方向地计算每一层的z、a、w、b的偏导数（梯度），从而更新参数。</li>
<li>每经过一次前向传播和反向传播之后，参数就更新一次，然后用新的参数再次循环上面的过程。这就是神经网络训练的整个过程。</li>
</ul>
<blockquote>
<p>高宽减半的时候通道数可以翻倍，同样一个像素的表示的信息要增加，所以要增加通道数，   </p>
</blockquote>
<h3 id="卷积层可学习的参数的个数"><a href="#卷积层可学习的参数的个数" class="headerlink" title="卷积层可学习的参数的个数"></a>卷积层可学习的参数的个数</h3><p>输入通道数 * 输出通道数 * kernel（卷积核）的大小&lt;3 * 3&gt;</p>
<h2 id="Alexnet"><a href="#Alexnet" class="headerlink" title="Alexnet"></a>Alexnet</h2><p>更深更大的Lenet</p>
<p>主要改进：</p>
<ul>
<li>丢弃法</li>
<li>ReLU</li>
<li>MaxPooling</li>
</ul>
<h3 id="计算机视觉方法论的改变："><a href="#计算机视觉方法论的改变：" class="headerlink" title="计算机视觉方法论的改变："></a>计算机视觉方法论的改变：</h3><p>从人工特征提取到端到端的学习，从原始的信息到最后的分类，神经网络一路走过去</p>
<h2 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h2><blockquote>
<p>完全不用全连接层</p>
</blockquote>
<h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><p>一个卷积层后跟两个全连接层</p>
<ul>
<li>步幅1，无填充，输出形状跟卷积层输出一样，1 * 1的卷积层起到全连接层的作用</li>
</ul>
<p>无全连接层，交替使用NiN块和步幅为2的最大池化层（宽高减半）逐步减少高宽和增大通道数</p>
<p>最后使用全局平均池化层得到输出，其输入通道数类别数</p>
<blockquote>
<p>NiN块使用卷积层加两个1 * 1卷积层</p>
<p>后者对每个像素增加了非线性性</p>
<p>NiN使用全局平局池化层来替代VGG和AlexNet中的全连接层</p>
<p>不容易过拟合，更少的参数个数</p>
</blockquote>
<h3 id="全局池化层"><a href="#全局池化层" class="headerlink" title="全局池化层"></a>全局池化层</h3><p>能够把模型复杂度降低，提升模型的泛化性，让精度变得更好。</p>
<p>让收敛变慢</p>
<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>5段，9个<code>inception</code>块</p>
<p><code>inception</code>不改变高宽，只改变通道数</p>
<p>MaxPool可以降低高宽</p>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>损失出现在最后，后面的层训练较快</li>
<li>数据在最底部<ul>
<li>底部的层训练较慢</li>
<li>底部曾一变化，所有都得跟着变</li>
<li>最后的那些曾需要重新学习多次</li>
<li>导致收敛变慢</li>
</ul>
</li>
</ul>
<h3 id="批量归一化层"><a href="#批量归一化层" class="headerlink" title="批量归一化层"></a>批量归一化层</h3><ul>
<li>可学习得参数为y和b</li>
<li>作用在<ul>
<li>全连接层和卷积层得输出上，激活函数前</li>
<li>全连接层和卷积层输入上</li>
</ul>
</li>
<li>对全连接层，作用在特征维</li>
<li>对卷积层，作用在通道维</li>
</ul>
<p>批量归一化固定小批量中得均值和方差，然后学习出适合得偏移和缩放</p>
<p>可以加速收敛速度，但一般不改变模型精度</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>ResNet沿用了VGG完整的3×33×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×33×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×11×1卷积层来将输入变换成需要的形状后再做相加运算。</p>
<p><code>residual</code>残留的；剩余的</p>
<ul>
<li>残差块使得很深得网络更加容易训练</li>
<li>甚至可以训练一千层的网络</li>
<li>残差网络对随后的深层神经网络设计产生了深远影响，无论是卷积类网络还是全连接网络</li>
</ul>
<h3 id="残差块结构："><a href="#残差块结构：" class="headerlink" title="残差块结构："></a>残差块结构：</h3><p><img src="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/resnet_block.png" alt="resnet_block"></p>
<p>​         </p>
<h2 id="单机多卡并行"><a href="#单机多卡并行" class="headerlink" title="单机多卡并行"></a>单机多卡并行</h2><ul>
<li>一台机器可以安装多个GPU</li>
<li>在训练和预测的时候，我们将一个小批量计算切分到多个GPU上来达到加速目的</li>
<li>常用的切分方案有<ul>
<li>数据并行</li>
<li>模型并行</li>
<li>通道并行（数据+模型并行）</li>
</ul>
</li>
</ul>
<h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>将小批量分成n块，每个GPU拿到完整参数计算一块数据的梯度</p>
<p>通常性能更好</p>
<h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3><p>将模型分成n块，每个GPU拿到一块模型计算它的前向和后向结果，通常用于模型大到放不下</p>
<p>当一个模型能用单卡计算时，通常使用数据并行拓展到多卡上，模型并行则用在超大模型上</p>
<h2 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h2><p>数据增广通过变形数据来获取多样性从而使得模型泛化性能更好，常见图片增广包括反转，切割，变色</p>
<p>理论上，原始数据的多样性足够好，就用不着做增广</p>
<p>增广没有改变分布，可能是增大了方差&gt;</p>
<h2 id="微调（fine-tuning）"><a href="#微调（fine-tuning）" class="headerlink" title="微调（fine-tuning）"></a>微调（fine-tuning）</h2><blockquote>
<p>迁移学习的常规技巧</p>
</blockquote>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><ul>
<li>一个神经网络一般可以分成两块<ul>
<li>特征抽取将原始像素变成容易线性分割的特征</li>
<li>线性分类器来做分类</li>
</ul>
</li>
</ul>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>微调包括以下四个步骤</p>
<ol>
<li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即<em>源模型</em>。</li>
<li>创建一个新的神经网络模型，即<em>目标模型</em>。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。</li>
</ol>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li>是一个目标数据集上的正常训练任务</li>
<li>使用更强的正则化<ul>
<li>使用更小的学习率</li>
<li>使用更少的数据迭代</li>
</ul>
</li>
<li>源数据集远复杂于目标数据，通常微调效果更好</li>
</ul>
<h3 id="重用分类器权重"><a href="#重用分类器权重" class="headerlink" title="重用分类器权重"></a>重用分类器权重</h3><ul>
<li>源数据集可能也有目标数据中的部分标号</li>
<li>可以使用预训练好模型分类器中对应标号对应的向量来做初始化</li>
</ul>
<h3 id="固定一些层"><a href="#固定一些层" class="headerlink" title="固定一些层"></a>固定一些层</h3><ul>
<li><p>神经网络通常学习有层次的特征表示</p>
<ul>
<li>低层次的特征更加通用</li>
<li>高层次的特征则更跟数据集相关</li>
</ul>
</li>
<li><p>可以固定底部的一些层的参数，不参与更新</p>
<blockquote>
<p>模型复杂度变低</p>
</blockquote>
</li>
<li><p>更强的正则</p>
</li>
</ul>
<blockquote>
<p>微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来完成提升精度</p>
<p>预训练模型质量很重要</p>
<p>微调通常速度更快，精度更高</p>
</blockquote>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><blockquote>
<p>物体检测识别图片里的多个物体的类别和位置</p>
<p>位置通常用边缘框表示</p>
</blockquote>
<h3 id="边缘框"><a href="#边缘框" class="headerlink" title="边缘框"></a>边缘框</h3><p>一个边缘框可以通过4个数字定义</p>
<p><img src="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/edge_dot.png"></p>
<blockquote>
<p>标注的成本很高</p>
</blockquote>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li>每行表示一个物体<ul>
<li>图片文件名，物体类别，边缘框</li>
</ul>
</li>
<li>COCO（cocodataset.org）<ul>
<li>80物体，330k图片，1.5M物体</li>
</ul>
</li>
</ul>
<h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2><p>一类目标检测算法是基于锚框</p>
<ul>
<li>提出多个被称为锚框的区域（边缘框）</li>
<li>预测每个锚框里是否含有关注的物体</li>
<li>如果是，预测从这个锚框到真实边缘框的偏移</li>
</ul>
<h3 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h3><blockquote>
<p>交并比</p>
</blockquote>
<h3 id="赋予锚框标号"><a href="#赋予锚框标号" class="headerlink" title="赋予锚框标号"></a>赋予锚框标号</h3><ul>
<li>每个锚框是一个训练样本</li>
<li>将每个锚框， 要么标注成背景，要么关联上一个真实边缘框</li>
<li>我们可能会生成大量的锚框，这个导致大量的负类样本</li>
</ul>
<h3 id="使用非极大值抑制（NMS）输出"><a href="#使用非极大值抑制（NMS）输出" class="headerlink" title="使用非极大值抑制（NMS）输出"></a>使用非极大值抑制（NMS）输出</h3><ul>
<li>每个锚框预测一个边缘框</li>
<li>NMS可以合并相似的预测<ul>
<li>选中非背景类的最大预测值</li>
<li>去掉所有其他和它IoU值大于西塔的预测</li>
<li>重复上述过程直到所有预测要么被选中，要么被丢掉</li>
</ul>
</li>
</ul>
<blockquote>
<p>预测值：对于那个类的softmax置信值</p>
</blockquote>
<p>在预测时，使用NMS来去掉冗余的预测</p>
<h2 id="编码器-解码器"><a href="#编码器-解码器" class="headerlink" title="编码器&amp;解码器"></a>编码器&amp;解码器</h2><h3 id="重新考察CNN"><a href="#重新考察CNN" class="headerlink" title="重新考察CNN"></a>重新考察CNN</h3><ul>
<li>编码器：将输入编程成中间表达形式（特征）</li>
<li>解码器：将中间表示解码成输出</li>
</ul>
<h3 id="重新考察RNN"><a href="#重新考察RNN" class="headerlink" title="重新考察RNN"></a>重新考察RNN</h3><ul>
<li>编码器：将文本表示成向量</li>
<li>解码器：：向量表示成输出</li>
</ul>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>一个模型被分为两块</p>
<p>编码器处理输出</p>
<p>解码器生成输出</p>
<p>其中Decoder也可以拿到输入</p>
<p><img src="https://raw.githubusercontent.com/Dra-Tammer/blog_pic/main/post_img/encoderDecoder.png"></p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h2 id="代码模板"><a href="#代码模板" class="headerlink" title="代码模板"></a>代码模板</h2><h3 id="d2l图片不显示"><a href="#d2l图片不显示" class="headerlink" title="d2l图片不显示"></a>d2l图片不显示</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示图像的代码末尾加上</span></span><br><span class="line">d2l.plt.show();</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络参量进行反馈时，梯度是累计计算而不是被替换，但在处理每一个batch时并不需要与其他batch的梯度混合起来计算，因此对每个batch调用一遍zero_grad将参数梯度置0</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="comment"># 将输入数据，标签加载到内存</span></span><br><span class="line">X, y = X.to(device), y.to(device)</span><br><span class="line"><span class="comment"># 前向传播计算预测值</span></span><br><span class="line">y_hat = net(X)</span><br><span class="line"><span class="comment"># 计算当前损失</span></span><br><span class="line">l = loss(y_hat, y)</span><br><span class="line"><span class="comment"># 反向传播计算精度</span></span><br><span class="line">l.backward()</span><br><span class="line"><span class="comment"># 更新所有参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></tbody></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag"><i class="fa fa-tag"></i> 技术</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/97340e65.html" rel="prev" title="pytorch_lea">
                  <i class="fa fa-angle-left"></i> pytorch_lea
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/f7d57928.html" rel="next" title="reinforcement_learning">
                  reinforcement_learning <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2022 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">tammer</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">69k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:10</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>


    <span id="timeDate">载入天数...</span>

    <script>
    var now = new Date();

    function createtime() {
        //此处修改你的建站时间或者网站上线时间
        var create_time = '<%- theme.running_time.create_time %>';
        var grt = new Date("08/05/2023 12:00:00");
        now.setTime(now.getTime() + 250);
        days = (now - grt) / 1000 / 60 / 60 / 24;
        dnum = Math.floor(days);
        hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum);
        hnum = Math.floor(hours);
        if (String(hnum).length == 1) {
            hnum = "0" + hnum;
        }
        minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes);
        if (String(mnum).length == 1) {
            mnum = "0" + mnum;
        }
        seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds);
        if (String(snum).length == 1) {
            snum = "0" + snum;
        }
        document.getElementById("timeDate").innerHTML = "This world has been running for " + dnum + " 天 " + hnum + " 小时 " + mnum + " 分 " + snum + " 秒";

    }
    setInterval("createtime()", 250);
    </script>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Dra-Tammer/blog_comments_db","issue_term":"title","theme":"github-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
